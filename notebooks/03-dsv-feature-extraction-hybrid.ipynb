{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"kernelVersion","sourceId":281045302}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: imports & corrected skimage imports\nimport os, json, time\nfrom pathlib import Path\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# FIXED imports (greycomatrix → graycomatrix, greycoprops → graycoprops)\nfrom skimage.feature import local_binary_pattern, graycomatrix, graycoprops\nfrom skimage.measure import shannon_entropy\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Correct path to your preprocessing outputs\n# ---------------------- IMPORTANT ----------------------\n# Change to YOUR dataset folder printed by \"os.listdir('/kaggle/input')\"\nOUT_ROOT = Path(\"/kaggle/input/2-preprocessing/artifacts/preprocessed_fast\")\n# -------------------------------------------------------\n\nSTRICT_DIR = OUT_ROOT / \"strict\"\nCNN_DIR    = OUT_ROOT / \"cnn\"\n\nDSV_OUT = Path(\"artifacts/dsv_features\")   # output directory\nDSV_OUT.mkdir(parents=True, exist_ok=True)\n\nsplits = [\"train\", \"validation\", \"test\"]\n\nprint(\"Using OUT_ROOT:\", OUT_ROOT)\nprint(\"Strict exists:\", STRICT_DIR.exists())\nprint(\"CNN exists:\", CNN_DIR.exists())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: helpers\ndef read_image(p):\n    img = cv2.imread(str(p))\n    if img is None:\n        return None\n    return img\n\ndef to_gray(img):\n    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\ndef safe_resize(img, size=(224,224)):\n    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n\ndef extract_grid_patches(img, num_patches=4, patch_size=(160,160)):\n    # grid sampling: 2x2 grid if num_patches=4; if num_patches>4, sample random\n    h,w = img.shape[:2]\n    patches = []\n    if num_patches == 1:\n        cx, cy = w//2, h//2\n        x1 = max(0, cx - patch_size[0]//2); y1 = max(0, cy - patch_size[1]//2)\n        patches.append(img[y1:y1+patch_size[1], x1:x1+patch_size[0]])\n        return patches\n    # attempt 2x2 grid positions\n    rows = int(np.sqrt(num_patches))\n    cols = rows\n    if rows*cols < num_patches:\n        cols += 1\n    x_steps = np.linspace(0, w-patch_size[0], cols, dtype=int)\n    y_steps = np.linspace(0, h-patch_size[1], rows, dtype=int)\n    for y in y_steps:\n        for x in x_steps:\n            if len(patches) >= num_patches: break\n            p = img[y:y+patch_size[1], x:x+patch_size[0]]\n            if p.shape[0] == patch_size[1] and p.shape[1] == patch_size[0]:\n                patches.append(p)\n    # if still fewer, sample random\n    while len(patches) < num_patches:\n        rx = np.random.randint(0, max(1, w-patch_size[0]+1))\n        ry = np.random.randint(0, max(1, h-patch_size[1]+1))\n        p = img[ry:ry+patch_size[1], rx:rx+patch_size[0]]\n        patches.append(p)\n    return patches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:03:01.557841Z","iopub.execute_input":"2025-11-22T11:03:01.558076Z","iopub.status.idle":"2025-11-22T11:03:01.568804Z","shell.execute_reply.started":"2025-11-22T11:03:01.558058Z","shell.execute_reply":"2025-11-22T11:03:01.568077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: classical features — FULL, CLEAN, CORRECTED VERSION\n\ndef lbp_hist_features(gray, P=8, R=1):\n    lbp = local_binary_pattern(gray, P, R, method=\"uniform\")\n    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, int(lbp.max())+2), density=True)\n    return hist\n\ndef glcm_features(gray, distances=[1], angles=[0], levels=8):\n    imgq = np.floor(gray / (256/levels)).astype('uint8')\n\n    glcm = graycomatrix(\n        imgq,\n        distances=distances,\n        angles=angles,\n        levels=levels,\n        symmetric=True,\n        normed=True\n    )\n\n    feats = {}\n    props = ['contrast','dissimilarity','homogeneity','energy','correlation']\n    for p in props:\n        try:\n            feats[p] = float(graycoprops(glcm, p).mean())\n        except:\n            feats[p] = 0.0\n    return feats\n\ndef fft_bandpower_features(gray):\n    f = np.fft.fft2(gray.astype(float))\n    fshift = np.fft.fftshift(f)\n    mag = np.abs(fshift)\n\n    h,w = gray.shape\n    cy, cx = h//2, w//2\n    r = np.sqrt((np.arange(h)[:,None] - cy)**2 + (np.arange(w)[None,:] - cx)**2)\n    r = r / r.max()\n\n    low = mag[r<=0.1].mean() if (r<=0.1).any() else 0.0\n    mid = mag[(r>0.1)&(r<=0.4)].mean() if ((r>0.1)&(r<=0.4)).any() else 0.0\n    high = mag[r>0.4].mean() if (r>0.4).any() else 0.0\n    total = low + mid + high if (low+mid+high)>0 else 1.0\n\n    return {\n        'fft_low': float(low/total),\n        'fft_mid': float(mid/total),\n        'fft_high': float(high/total)\n    }\n\ndef edge_features(gray):\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n    mag = np.sqrt(sobelx*sobelx + sobely*sobely)\n    sobel_mean = float(np.mean(mag))\n\n    canny = cv2.Canny(gray, 50, 150)\n    canny_ratio = float(canny.mean() / 255.0)\n\n    return {\n        'sobel_mean': sobel_mean,\n        'canny_ratio': canny_ratio\n    }\n\ndef keypoint_counts(gray):\n    orb = cv2.ORB_create(nfeatures=1000)\n    kp_orb = orb.detect(gray, None)\n    orb_count = len(kp_orb)\n\n    try:\n        ak = cv2.AKAZE_create()\n        kp_ak = ak.detect(gray, None)\n        ak_count = len(kp_ak)\n    except:\n        ak_count = 0\n\n    return {\n        'orb_kp': orb_count,\n        'akaze_kp': ak_count\n    }\n\ndef entropy_feature(gray):\n    return float(shannon_entropy(gray))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:03:01.569600Z","iopub.execute_input":"2025-11-22T11:03:01.569887Z","iopub.status.idle":"2025-11-22T11:03:01.592166Z","shell.execute_reply.started":"2025-11-22T11:03:01.569859Z","shell.execute_reply":"2025-11-22T11:03:01.591375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: CNN patch embedding extractor\n# We will use MobileNetV2 (imagenet) as a fixed feature extractor (no training here).\nIMG_SIZE = (160,160)\nbase_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMG_SIZE[1], IMG_SIZE[0], 3))\nprint(\"Loaded MobileNetV2 base for patch embeddings.\")\n\ndef patch_embeddings(img, num_patches=4):\n    patches = extract_grid_patches(img, num_patches=num_patches, patch_size=IMG_SIZE)\n    emb_list = []\n    for p in patches:\n        # ensure 3 channels\n        if p.shape[2] == 1:\n            p = cv2.cvtColor(p, cv2.COLOR_GRAY2BGR)\n        p_resized = cv2.resize(p, IMG_SIZE)\n        x = img_to_array(p_resized)\n        x = preprocess_input(x)\n        x = np.expand_dims(x, axis=0)\n        feat = base_model.predict(x, verbose=0)\n        emb_list.append(feat.flatten())\n    emb_arr = np.vstack(emb_list)  # shape (num_patches, feat_dim)\n    # return aggregated stats\n    return {\n        'patch_emb_mean': emb_arr.mean(axis=0),\n        'patch_emb_std': emb_arr.std(axis=0),\n        'patch_count': emb_arr.shape[0]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:03:01.592932Z","iopub.execute_input":"2025-11-22T11:03:01.593193Z","iopub.status.idle":"2025-11-22T11:03:05.301231Z","shell.execute_reply.started":"2025-11-22T11:03:01.593171Z","shell.execute_reply":"2025-11-22T11:03:05.300562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: driver to extract features for a single image (returns dict)\ndef extract_image_features(img_path, num_patches=4):\n    img = read_image(img_path)\n    if img is None:\n        return None\n    gray = to_gray(img)\n    feats = {}\n    # basic metadata\n    feats['image'] = str(img_path)\n    feats['height'], feats['width'] = img.shape[:2]\n    # classical\n    feats.update(fft_bandpower_features(gray))\n    feats.update(edge_features(gray))\n    feats.update(keypoint_counts(gray))\n    feats['entropy'] = entropy_feature(gray)\n    # LBP summary (we'll store first 10 histogram bins; pad if shorter)\n    lbp_hist = lbp_hist_features(gray, P=8, R=1)\n    for i in range(16):\n        feats[f'lbp_{i}'] = float(lbp_hist[i]) if i < len(lbp_hist) else 0.0\n    # GLCM\n    glcm = glcm_features(gray, distances=[1], angles=[0], levels=8)\n    for k,v in glcm.items():\n        feats[f'glcm_{k}'] = v\n    # patch embeddings (aggregated mean/std) -> these are vectors; we'll save reduced stats\n    emb = patch_embeddings(img, num_patches=num_patches)\n    # to keep CSV small, store embedding mean length stats (mean of embedding values and std)\n    feats['patch_emb_mean_mean'] = float(np.mean(emb['patch_emb_mean']))\n    feats['patch_emb_mean_std']  = float(np.std(emb['patch_emb_mean']))\n    feats['patch_emb_std_mean']  = float(np.mean(emb['patch_emb_std']))\n    feats['patch_emb_std_std']   = float(np.std(emb['patch_emb_std']))\n    feats['patch_count'] = int(emb['patch_count'])\n    return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:03:05.302838Z","iopub.execute_input":"2025-11-22T11:03:05.303083Z","iopub.status.idle":"2025-11-22T11:03:05.311005Z","shell.execute_reply.started":"2025-11-22T11:03:05.303065Z","shell.execute_reply":"2025-11-22T11:03:05.310041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: batch run and CSV output (label-preserving version)\n\nOUT_DIR = DSV_OUT\nLOG_JSON = OUT_DIR / \"dsv_extract_log.json\"\n\nif LOG_JSON.exists():\n    with open(LOG_JSON,\"r\") as f:\n        processed_log = json.load(f)\nelse:\n    processed_log = {}\n\nfor split in splits:\n    print(f\"\\nProcessing split: {split}\")\n\n    input_dir = CNN_DIR / split\n\n    if not input_dir.exists():\n        print(\"Missing directory:\", input_dir)\n        continue\n\n    print(\"Input dir:\", input_dir)\n    files = sorted(input_dir.rglob(\"*.jpg\"))\n    print(\"Files:\", len(files))\n\n    rows = []\n    csv_path = OUT_DIR / f\"{split}.csv\"\n\n    for p in tqdm(files):\n        key = f\"{split}/{p.name}\"\n        if key in processed_log:\n            continue\n\n        feats = extract_image_features(p, num_patches=4)\n        if feats is None:\n            processed_log[key] = {\"status\":\"read_fail\"}\n            continue\n\n        # Extract true label from folder name\n        true_label = p.parent.name     # e.g., denomination_10, denomination_fake\n\n        feats['source'] = true_label \n        feats['split'] = split\n        rows.append(feats)\n        processed_log[key] = {\"status\":\"done\"}\n\n        if len(rows) >= 200:\n            df = pd.DataFrame(rows)\n            if csv_path.exists():\n                df.to_csv(csv_path, mode='a', header=False, index=False)\n            else:\n                df.to_csv(csv_path, index=False)\n            rows = []\n            with open(LOG_JSON,\"w\") as f:\n                json.dump(processed_log, f, indent=2)\n\n    # final flush\n    if rows:\n        df = pd.DataFrame(rows)\n        if csv_path.exists():\n            df.to_csv(csv_path, mode='a', header=False, index=False)\n        else:\n            df.to_csv(csv_path, index=False)\n\n    with open(LOG_JSON,\"w\") as f:\n        json.dump(processed_log, f, indent=2)\n\n    print(\"Saved:\", csv_path)\n\nprint(\"DSV feature extraction complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:03:05.312040Z","iopub.execute_input":"2025-11-22T11:03:05.312363Z","iopub.status.idle":"2025-11-22T12:47:59.283939Z","shell.execute_reply.started":"2025-11-22T11:03:05.312332Z","shell.execute_reply":"2025-11-22T12:47:59.283036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: combine and summary\ndfs = []\nfor split in splits:\n    p = DSV_OUT / f\"{split}.csv\"\n    if p.exists():\n        df = pd.read_csv(p)\n        dfs.append(df)\nif len(dfs)>0:\n    all_df = pd.concat(dfs, ignore_index=True)\n    all_csv = DSV_OUT / \"all_dsv_features.csv\"\n    all_df.to_csv(all_csv, index=False)\n    print(\"Combined CSV saved to:\", all_csv)\n    print(\"Total rows:\", len(all_df))\nelse:\n    print(\"No per-split CSVs found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T12:51:01.628488Z","iopub.execute_input":"2025-11-22T12:51:01.628802Z","iopub.status.idle":"2025-11-22T12:51:02.314899Z","shell.execute_reply.started":"2025-11-22T12:51:01.628778Z","shell.execute_reply":"2025-11-22T12:51:02.314244Z"}},"outputs":[],"execution_count":null}]}